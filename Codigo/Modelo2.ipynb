{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"hMyD0udzzRCg","colab":{"base_uri":"https://localhost:8080/"},"outputId":"037fed0b-8e15-4a13-e7f8-c17bb01b0742"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow==2.10.0 in /usr/local/lib/python3.10/dist-packages (2.10.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.6.3)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (24.3.25)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.68.1)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (3.12.1)\n","Requirement already satisfied: keras<2.11,>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (2.10.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.1.2)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (18.1.1)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.26.4)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (24.2)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (3.19.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (75.1.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.16.0)\n","Requirement already satisfied: tensorboard<2.11,>=2.10 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (2.10.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (0.37.1)\n","Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (2.10.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (2.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.17.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.10.0) (0.45.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.4.6)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.7)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.32.3)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.6.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (1.8.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.1.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (5.5.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.4.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2024.8.30)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.0.2)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.6.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.2.2)\n"]}],"source":["!pip install tensorflow==2.10.0"]},{"cell_type":"markdown","source":["#Imports"],"metadata":{"id":"ja-8030zvll4"}},{"cell_type":"code","source":["import os\n","import librosa\n","import numpy as np\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","import matplotlib.pyplot as plt\n","import tarfile"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qocgQ9JZK7f9","outputId":"cc741377-065c-4020-c68e-dfa6041767dc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["ERROR:absl:cannot import name 'builder' from 'google.protobuf.internal' (/usr/local/lib/python3.10/dist-packages/google/protobuf/internal/__init__.py)\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow_datasets/__init__.py\", line 79, in <module>\n","    from tensorflow_datasets import rlds  # pylint: disable=g-bad-import-order\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow_datasets/rlds/__init__.py\", line 21, in <module>\n","    from tensorflow_datasets.rlds import envlogger_reader\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow_datasets/rlds/envlogger_reader.py\", line 21, in <module>\n","    from tensorflow_datasets.core.utils.lazy_imports_utils import tree\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow_datasets/core/__init__.py\", line 21, in <module>\n","    from tensorflow_datasets.core import community\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow_datasets/core/community/__init__.py\", line 19, in <module>\n","    from tensorflow_datasets.core.community.huggingface_wrapper import mock_builtin_to_use_gfile\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow_datasets/core/community/huggingface_wrapper.py\", line 31, in <module>\n","    from tensorflow_datasets.core import dataset_builder\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow_datasets/core/dataset_builder.py\", line 158, in <module>\n","    class DatasetBuilder(registered.RegisteredDataset):\n","  File \"/usr/local/lib/python3.10/dist-packages/etils/epy/lazy_imports_utils.py\", line 109, in __getattr__\n","    return getattr(self._module, name)\n","  File \"/usr/lib/python3.10/functools.py\", line 981, in __get__\n","    val = self.func(instance)\n","  File \"/usr/local/lib/python3.10/dist-packages/etils/epy/lazy_imports_utils.py\", line 70, in _module\n","    module = importlib.import_module(self.module_name)\n","  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n","    return _bootstrap._gcd_import(name[level:], package, level)\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow_datasets/core/registered.py\", line 36, in <module>\n","    import tensorflow_datasets.core.logging as _tfds_logging\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow_datasets/core/logging/__init__.py\", line 29, in <module>\n","    from tensorflow_datasets.core.logging import logging_logger\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow_datasets/core/logging/logging_logger.py\", line 21, in <module>\n","    from tensorflow_datasets.core import decode\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow_datasets/core/decode/__init__.py\", line 22, in <module>\n","    from tensorflow_datasets.core.decode.partial_decode import PartialDecoding\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow_datasets/core/decode/partial_decode.py\", line 21, in <module>\n","    from tensorflow_datasets.core import features as features_lib\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow_datasets/core/features/__init__.py\", line 22, in <module>\n","    from tensorflow_datasets.core.features.audio_feature import Audio\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow_datasets/core/features/audio_feature.py\", line 33, in <module>\n","    from tensorflow_datasets.core.features import feature as feature_lib\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow_datasets/core/features/feature.py\", line 36, in <module>\n","    from tensorflow_datasets.core.proto import feature_pb2\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow_datasets/core/proto/__init__.py\", line 18, in <module>\n","    from tensorflow_datasets.core.proto import dataset_info_generated_pb2 as dataset_info_pb2  # pylint: disable=line-too-long\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow_datasets/core/proto/dataset_info_generated_pb2.py\", line 25, in <module>\n","    from google.protobuf.internal import builder as _builder\n","ImportError: cannot import name 'builder' from 'google.protobuf.internal' (/usr/local/lib/python3.10/dist-packages/google/protobuf/internal/__init__.py)\n"]}]},{"cell_type":"markdown","source":["#Para Cargar el Dataset desde Google Colab\n","\n","  (Antes de ejecutar las estas celdas de código)\n","\n","1. Descarga el dataset de LJSpeech de esta URL: https://keithito.com/LJ-Speech-Dataset/\n","2. El archivo se descargará con la extensión \".tar.bz2\". Bueno pues descomprime el archivo para que solo tengas la extensión \".tar\"\n","3. Sube el archivo .tar a tu unidad de Google Drive (Necesitarás como 3Gb libres)"],"metadata":{"id":"Rj0L0p2VvfWx"}},{"cell_type":"markdown","source":["Monta en el sistema de archivos de Colab tu unidad de Google Drive"],"metadata":{"id":"tOlnTnZLwSnW"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"CRJdd2Dgvbny","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d31ed7df-aad1-494d-8b4a-5554e16f99dc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["Descomprime el archivo .tar de tu unidad en Google Drive a una carpeta nueva llamada dataset"],"metadata":{"id":"KfLO2W6OwcJ0"}},{"cell_type":"code","source":["# Nombre del archivo .tar y la carpeta de destino\n","nombre_archivo_tar = '/content/drive/MyDrive/LJSpeech-1.1.tar'\n","carpeta_destino = '/content/dataset'\n","\n","# Abrir el archivo .tar y extraer su contenido\n","with tarfile.open(nombre_archivo_tar, 'r') as archivo_tar:\n","    archivo_tar.extractall(path=carpeta_destino)\n","\n","print('Archivo descomprimido correctamente.')\n"],"metadata":{"id":"3jiJ8RdvLVZQ","colab":{"base_uri":"https://localhost:8080/","height":356},"outputId":"f5e6faab-8e76-4235-dbdf-5e112fc455df"},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-4e3bca0a8372>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Abrir el archivo .tar y extraer su contenido\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnombre_archivo_tar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0marchivo_tar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0marchivo_tar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcarpeta_destino\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Archivo descomprimido correctamente.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/tarfile.py\u001b[0m in \u001b[0;36mextractall\u001b[0;34m(self, path, members, numeric_owner, filter)\u001b[0m\n\u001b[1;32m   2284\u001b[0m                 \u001b[0;31m# extracting contents can reset mtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2285\u001b[0m                 \u001b[0mdirectories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarinfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2286\u001b[0;31m             self._extract_one(tarinfo, path, set_attrs=not tarinfo.isdir(),\n\u001b[0m\u001b[1;32m   2287\u001b[0m                               numeric_owner=numeric_owner)\n\u001b[1;32m   2288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/tarfile.py\u001b[0m in \u001b[0;36m_extract_one\u001b[0;34m(self, tarinfo, path, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2348\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2349\u001b[0;31m             self._extract_member(tarinfo, os.path.join(path, tarinfo.name),\n\u001b[0m\u001b[1;32m   2350\u001b[0m                                  \u001b[0mset_attrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset_attrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2351\u001b[0m                                  numeric_owner=numeric_owner)\n","\u001b[0;32m/usr/lib/python3.10/tarfile.py\u001b[0m in \u001b[0;36m_extract_member\u001b[0;34m(self, tarinfo, targetpath, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2431\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misreg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2432\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2433\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/tarfile.py\u001b[0m in \u001b[0;36mmakefile\u001b[0;34m(self, tarinfo, targetpath)\u001b[0m\n\u001b[1;32m   2483\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2485\u001b[0;31m                 \u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReadError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2487\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmakeunknown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/tarfile.py\u001b[0m in \u001b[0;36mcopyfileobj\u001b[0;34m(src, dst, length, exception, bufsize)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremainder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdivmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unexpected end of data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["Código necesario para que el modelo entienda el DataSet"],"metadata":{"id":"RcG2dbCuw2jH"}},{"cell_type":"code","source":["# Configuración\n","data_dir = '/content/dataset/LJSpeech-1.1'\n","wav_dir = os.path.join(data_dir, 'wavs')\n","metadata_file = os.path.join(data_dir, 'metadata.csv')\n","sample_rate = 22050\n","max_length = 500  # Ajusta según tus necesidades\n","\n","# Función para cargar y preprocesar audio\n","def load_audio(file_path, sample_rate):\n","    audio, _ = librosa.load(file_path, sr=sample_rate)\n","    return audio\n","\n","# Función para convertir audio a espectrograma\n","def audio_to_spectrogram(audio, sample_rate):\n","    spectrogram = librosa.feature.melspectrogram(y=audio, sr=sample_rate, n_mels=128)\n","    spectrogram_db = librosa.power_to_db(spectrogram, ref=np.max)\n","    return spectrogram_db.T\n","\n","# Cargar transcripciones y audios\n","transcriptions = []\n","spectrograms = []\n","with open(metadata_file, 'r') as f:\n","    for line in f:\n","        parts = line.strip().split('|')\n","        file_id = parts[0]\n","        transcription = parts[2]\n","        transcriptions.append(transcription)\n","\n","        # Cargar y procesar audio\n","        wav_path = os.path.join(wav_dir, f'{file_id}.wav')\n","        audio = load_audio(wav_path, sample_rate)\n","        spectrogram = audio_to_spectrogram(audio, sample_rate)\n","\n","        # Padding o truncamiento\n","        if spectrogram.shape[0] > max_length:\n","            spectrogram = spectrogram[:max_length, :]\n","        else:\n","            pad_width = max_length - spectrogram.shape[0]\n","            spectrogram = np.pad(spectrogram, ((0, pad_width), (0, 0)), mode='constant')\n","\n","        spectrograms.append(spectrogram)\n","\n","# Convertir a tensores de TensorFlow\n","transcriptions = np.array(transcriptions)\n","spectrograms = np.array(spectrograms)\n","\n","# Tokenización de transcripciones (simple ejemplo)\n","tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\n","tokenizer.fit_on_texts(transcriptions)\n","sequences = tokenizer.texts_to_sequences(transcriptions)\n","sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_length)\n","\n","# Dividir en conjuntos de entrenamiento y validación\n","train_ratio = 0.8\n","num_train = int(len(sequences) * train_ratio)\n","\n","x_train = sequences[:num_train]\n","y_train = spectrograms[:num_train]\n","x_val = sequences[num_train:]\n","y_val = spectrograms[num_train:]"],"metadata":{"id":"SQzts_dtjgiF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Modelo"],"metadata":{"id":"kmPkGNrrw9Kl"}},{"cell_type":"code","source":["class TextEncoder(tf.keras.Model):\n","    def __init__(self, vocab_size, embed_dim):\n","        super(TextEncoder, self).__init__()\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embed_dim)\n","\n","    def call(self, x):\n","        return self.embedding(x)\n","\n","class SpectrogramDecoder(tf.keras.Model):\n","    def __init__(self, target_dim):\n","        super(SpectrogramDecoder, self).__init__()\n","        self.dense = tf.keras.Sequential([\n","            tf.keras.layers.Dense(128, activation='relu'),\n","            tf.keras.layers.Dense(target_dim, activation=None)  # No activación para espectrogramas sin normalizar\n","        ])\n","\n","    def call(self, x):\n","        return self.dense(x)\n","\n","class TextToSpectrogram(tf.keras.Model):\n","    def __init__(self, vocab_size, embed_dim, target_dim):\n","        super(TextToSpectrogram, self).__init__()\n","        self.encoder = TextEncoder(vocab_size, embed_dim)\n","        self.decoder = SpectrogramDecoder(target_dim)\n","\n","    def call(self, x):\n","        x = self.encoder(x)\n","        return self.decoder(x)\n","\n","class TransformerBlock(tf.keras.layers.Layer):\n","    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n","        super(TransformerBlock, self).__init__()\n","        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.ffn = tf.keras.Sequential(\n","            [tf.keras.layers.Dense(ff_dim, activation=\"relu\"), tf.keras.layers.Dense(embed_dim)]\n","        )\n","        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = tf.keras.layers.Dropout(rate)\n","        self.dropout2 = tf.keras.layers.Dropout(rate)\n","\n","    def call(self, inputs, training):\n","        attn_output = self.att(inputs, inputs)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(inputs + attn_output)\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        return self.layernorm2(out1 + ffn_output)\n","\n","\n","\n","def create_transformer_model(vocab_size, embed_dim, num_heads, ff_dim, target_dim, max_length):\n","    input_text = tf.keras.layers.Input(shape=(max_length,), name=\"text_input\")\n","    embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_dim)(input_text)\n","\n","    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n","    x = transformer_block(embedding_layer,training=False)\n","\n","    flatten_layer = tf.keras.layers.Flatten()(x)\n","    dense_layer = tf.keras.layers.Dense(128, activation='relu')(flatten_layer)\n","    output_layer = tf.keras.layers.Dense(target_dim, activation=None)(dense_layer)\n","\n","    # Cambiar la capa Reshape para que coincida con la forma objetivo (500, 128)\n","    output_spectrogram = tf.keras.layers.Reshape((max_length, target_dim // max_length))(output_layer)\n","\n","    model = tf.keras.Model(inputs=input_text, outputs=output_spectrogram, name=\"TextToSpectrogramTransformer\")\n","    return model\n"],"metadata":{"id":"o5hc_6wVK9T8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Entrenamiento"],"metadata":{"id":"6_e7Jka5xAT8"}},{"cell_type":"code","source":["# Crear modelo\n","vocab_size = len(tokenizer.word_index) + 1\n","embed_dim = 128\n","num_heads = 4\n","ff_dim = 128\n","target_dim = spectrograms.shape[1] * spectrograms.shape[2]\n","\n","model = create_transformer_model(vocab_size, embed_dim, num_heads, ff_dim, target_dim, max_length)\n","model.compile(optimizer='adam', loss='mse')\n","model.summary()\n","\n","\n"],"metadata":{"id":"BJqscUBoLCez"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Entrenar modelo\n","model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=10, batch_size=32)\n","# Guarda el modelo en una ruta en Google Drive\n","model.save_weights('/content/drive/My Drive/pesos_mi_modelo.h5')"],"metadata":{"id":"AteAIc3H-NAf"},"execution_count":null,"outputs":[]}]}